{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3bab8a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_TRACING_v2\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"tutorial_chatbot\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d1b498cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c1bd23c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import utils\n",
    "utils.tracing_is_enabled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f9da36ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'url': 'https://www.youtube.com/watch?v=Iop2WmVyZjQ', 'content': 'This webinar provides a general overview of openBIS (https://openbis.ch/) is a combined data management platform, Electronic Laboratory Notebook, and Inventory Management System. openBIS helps scientists meet requirements from funding agencies, journals, and academic institutions to publish data according to the FAIR data principles. Scientists can use openBIS to document their daily experimental or computational work, store any related experimental raw and derived data, and link everything to'}, {'url': 'https://www.uzh.ch/blog/hbz/2019/11/20/how-to-use-openbis-as-electronic-lab-notebook/?lang=en', 'content': 'openBIS is a free and open-source data manager developed by the ETH Zurich and supports the full data life cycle from project inception, through'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "search = TavilySearchResults(max_results=2)\n",
    "search_results = search.invoke(\"What is openBIS and how to use it?\")\n",
    "print(search_results)\n",
    "# If we want, we can create other tools.\n",
    "# Once we have all the tools we want, we can put them in a list that we will reference later.\n",
    "tools = [search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b65879f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"qwen3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9f844422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user said \"hi!\" I should respond in a friendly and welcoming way. Let me make sure to acknowledge their greeting and offer assistance. Maybe start with a cheerful \"Hello!\" to match their enthusiasm. Then, ask how I can help them today. Keep it simple and open-ended so they feel comfortable sharing what they need. Let me check if there\\'s anything else I should consider. No, that should be good. Ready to respond.\\n</think>\\n\\nHello! 😊 How can I assist you today? Let me know if you have any questions or need help with something!'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = model.invoke([HumanMessage(content=\"hi!\")])\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "75483d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_tools = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d1849faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContentString: <think>\n",
      "Okay, the user said \"Hi!\" so they're probably just greeting me. I need to respond politely. Since there's no specific question or request, I don't need to use any tools here. I'll just reply with a friendly message to acknowledge their greeting. Let me make sure the response is welcoming and offers assistance if they need anything else.\n",
      "</think>\n",
      "\n",
      "Hello! How can I assist you today? 😊\n",
      "ToolCalls: []\n"
     ]
    }
   ],
   "source": [
    "response = model_with_tools.invoke([HumanMessage(content=\"Hi!\")])\n",
    "\n",
    "print(f\"ContentString: {response.content}\")\n",
    "print(f\"ToolCalls: {response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c214b948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContentString: Hello! How can I assist you today? 😊\n",
      "ToolCalls: []\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Clean up the response content from <think> tags\n",
    "response = model_with_tools.invoke([HumanMessage(content=\"Hi!\")])\n",
    "raw = response.content\n",
    "clean = re.sub(r\"<think>.*?</think>\", \"\", raw, flags=re.DOTALL).strip()\n",
    "\n",
    "print(f\"ContentString: {clean}\")\n",
    "print(f\"ToolCalls: {response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5519320e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContentString: \n",
      "ToolCalls: [{'name': 'tavily_search_results_json', 'args': {'query': 'current weather in SF'}, 'id': 'b2ac0639-1ce9-45d8-a00f-6a2a0b2d9505', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "response = model_with_tools.invoke([HumanMessage(content=\"What's the weather in SF?\")])\n",
    "\n",
    "print(f\"ContentString: {response.content}\")\n",
    "print(f\"ToolCalls: {response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eb8b2f",
   "metadata": {},
   "source": [
    "## Create the agent\n",
    "Now that we have defined the tools and the LLM, we can create the agent. We will be using LangGraph to construct the agent. Currently, we are using a high level interface to construct the agent, but the nice thing about LangGraph is that this high-level interface is backed by a low-level, highly controllable API in case you want to modify the agent logic.\n",
    "\n",
    "Now, we can initialize the agent with the LLM and the tools.\n",
    "\n",
    "Note that we are passing in the model, not model_with_tools. That is because create_react_agent will call .bind_tools for us under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8ff9fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_executor = create_react_agent(model, tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebb6a19",
   "metadata": {},
   "source": [
    "## Run the agent\n",
    "We can now run the agent with a few queries! Note that for now, these are all stateless queries (it won't remember previous interactions). Note that the agent will return the final state at the end of the interaction (which includes any inputs, we will see later on how to get only the outputs).\n",
    "\n",
    "First up, let's see how it responds when there's no need to call a tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1fdbcfc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi!', additional_kwargs={}, response_metadata={}, id='7b00d86e-55cc-4da0-b4fd-1540ec25b2bd'),\n",
       " SystemMessage(content='keep it short!', additional_kwargs={}, response_metadata={}, id='b9b11a64-cde9-456b-8230-9428de048ecc'),\n",
       " AIMessage(content='<think>\\nOkay, the user said \"hi!\". I need to respond appropriately. Since there\\'s no specific question or request, just a greeting, I should reply with a friendly greeting. No need to use any tools here. Let me make sure the response is welcoming and offers assistance. Something like \"Hello! How can I assist you today?\" That should cover it.\\n</think>\\n\\nHello! How can I assist you today?', additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-04-30T14:26:00.714144694Z', 'done': True, 'done_reason': 'stop', 'total_duration': 540469838, 'load_duration': 19735897, 'prompt_eval_count': 177, 'prompt_eval_duration': 17392020, 'eval_count': 86, 'eval_duration': 496203430, 'message': Message(role='assistant', content='<think>\\nOkay, the user said \"hi!\". I need to respond appropriately. Since there\\'s no specific question or request, just a greeting, I should reply with a friendly greeting. No need to use any tools here. Let me make sure the response is welcoming and offers assistance. Something like \"Hello! How can I assist you today?\" That should cover it.\\n</think>\\n\\nHello! How can I assist you today?', images=None, tool_calls=None)}, id='run-b300b73d-e505-4e49-910e-8e117917b56b-0', usage_metadata={'input_tokens': 177, 'output_tokens': 86, 'total_tokens': 263})]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "response = agent_executor.invoke({\"messages\": [HumanMessage(content=\"hi!\"), SystemMessage(content=\"keep it short!\")]})\n",
    "\n",
    "response[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eed42802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='whats openBIS?', additional_kwargs={}, response_metadata={}, id='ae4e282f-68c7-4c5f-8a8f-314295202db7'),\n",
       " AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-04-30T14:26:01.859899942Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1123549622, 'load_duration': 17727760, 'prompt_eval_count': 177, 'prompt_eval_duration': 16519213, 'eval_count': 188, 'eval_duration': 1086681415, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-972bf473-2df6-4ef8-9945-73a86e468426-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'openBIS'}, 'id': 'eaa7aea0-7e9d-454c-bb53-812de56b28c6', 'type': 'tool_call'}], usage_metadata={'input_tokens': 177, 'output_tokens': 188, 'total_tokens': 365}),\n",
       " ToolMessage(content='[{\"url\": \"https://sis.id.ethz.ch/services/rdm/openbis.html\", \"content\": \"openBIS is a combined FAIR data management platform, Electronic Laboratory Notebook (ELN) and Inventory Management System, under active development since 2007. Whilst originally developed primarily as a Laboratory Information Management System (LIMS) to manage and share large quantities of â\\x80\\x9comicsâ\\x80\\x9d data produced in collaborative Swiss systems biology projects, since 2013 the development has focused primarily on introducing an ELN and Inventory Management module to extend and facilitate usage [...] openBIS ELN and Inventory Management module\\\\n\\\\nopenBIS enables scientists to document their daily experimental or computation work, store any related experimental raw and derived data and link everything to materials, samples and protocols stored in the lab inventory. Visit the openBIS website for more information.\\\\n\\\\nopenBIS BigDataLink [...] also in small- to medium-size quantitative research laboratories. openBIS enables scientists to meet the ever increasing requirements from funding agencies, journals, and academic institutions to publish data according to the FAIR data principles â\\x80\\x93 according to which data should be Findable, Accessible, Interoperable and Reusable.\"}, {\"url\": \"https://www.uzh.ch/blog/hbz/2019/11/20/how-to-use-openbis-as-electronic-lab-notebook/?lang=en\", \"content\": \"openBIS is a free and open-source data manager developed by the ETH Zurich and supports the full data life cycle from project inception, through\"}]', name='tavily_search_results_json', id='cd32bb2c-1519-4dce-b335-3ea4f13d0814', tool_call_id='eaa7aea0-7e9d-454c-bb53-812de56b28c6', artifact={'query': 'openBIS', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://sis.id.ethz.ch/services/rdm/openbis.html', 'title': 'Data Management Software Platform openBIS - ETH:SIS', 'content': 'openBIS is a combined FAIR data management platform, Electronic Laboratory Notebook (ELN) and Inventory Management System, under active development since 2007. Whilst originally developed primarily as a Laboratory Information Management System (LIMS) to manage and share large quantities of â\\x80\\x9comicsâ\\x80\\x9d data produced in collaborative Swiss systems biology projects, since 2013 the development has focused primarily on introducing an ELN and Inventory Management module to extend and facilitate usage [...] openBIS ELN and Inventory Management module\\n\\nopenBIS enables scientists to document their daily experimental or computation work, store any related experimental raw and derived data and link everything to materials, samples and protocols stored in the lab inventory. Visit the openBIS website for more information.\\n\\nopenBIS BigDataLink [...] also in small- to medium-size quantitative research laboratories. openBIS enables scientists to meet the ever increasing requirements from funding agencies, journals, and academic institutions to publish data according to the FAIR data principles â\\x80\\x93 according to which data should be Findable, Accessible, Interoperable and Reusable.', 'score': 0.9210814, 'raw_content': None}, {'url': 'https://www.uzh.ch/blog/hbz/2019/11/20/how-to-use-openbis-as-electronic-lab-notebook/?lang=en', 'title': 'How to use OpenBis as electronic lab notebook - Universität Zürich', 'content': 'openBIS is a free and open-source data manager developed by the ETH Zurich and supports the full data life cycle from project inception, through', 'score': 0.91936076, 'raw_content': None}], 'response_time': 0.68}),\n",
       " AIMessage(content='<think>\\nOkay, the user asked, \"What is openBIS?\" I need to provide a clear and concise answer based on the search results. Let me look at the tool response.\\n\\nThe first result mentions that openBIS is a FAIR data management platform, ELN, and Inventory Management System. It started as a LIMS for omics data in systems biology projects but now focuses on ELN and Inventory. It helps scientists document experiments, store data, and link to materials and protocols. The second result adds that it\\'s free, open-source, developed by ETH Zurich, and supports the data life cycle. It also mentions compliance with FAIR principles.\\n\\nI should combine these points. Start by defining openBIS as a platform combining data management, ELN, and inventory. Mention its origins and evolution. Highlight its features like FAIR compliance, data lifecycle support, and use in research labs. Include the key stakeholders (ETH Zurich) and the benefits for scientists. Make sure to mention the website for more info. Avoid technical jargon, keep it straightforward. Check for clarity and flow. Ensure all key points from both results are covered without redundancy.\\n</think>\\n\\nopenBIS is a **FAIR data management platform** that combines an **Electronic Laboratory Notebook (ELN)**, **Inventory Management System**, and **Laboratory Information Management System (LIMS)**. Developed by ETH Zurich since 2007, it initially focused on managing large-scale \"omics\" data from systems biology projects but has since evolved to emphasize ELN and inventory modules. \\n\\nKey features include:\\n- Enabling scientists to document experiments, store raw/derived data, and link it to materials, samples, and protocols.\\n- Supporting the **full data life cycle** (from project inception to data sharing).\\n- Ensuring compliance with **FAIR data principles** (Findable, Accessible, Interoperable, Reusable).\\n- Being **open-source** and used in both large-scale research and smaller laboratories.\\n\\nFor more details, visit the [openBIS website](https://sis.id.ethz.ch/services/rdm/openbis.html).', additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-04-30T14:26:05.91462356Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2947964217, 'load_duration': 16552831, 'prompt_eval_count': 541, 'prompt_eval_duration': 261635767, 'eval_count': 432, 'eval_duration': 2650552262, 'message': Message(role='assistant', content='<think>\\nOkay, the user asked, \"What is openBIS?\" I need to provide a clear and concise answer based on the search results. Let me look at the tool response.\\n\\nThe first result mentions that openBIS is a FAIR data management platform, ELN, and Inventory Management System. It started as a LIMS for omics data in systems biology projects but now focuses on ELN and Inventory. It helps scientists document experiments, store data, and link to materials and protocols. The second result adds that it\\'s free, open-source, developed by ETH Zurich, and supports the data life cycle. It also mentions compliance with FAIR principles.\\n\\nI should combine these points. Start by defining openBIS as a platform combining data management, ELN, and inventory. Mention its origins and evolution. Highlight its features like FAIR compliance, data lifecycle support, and use in research labs. Include the key stakeholders (ETH Zurich) and the benefits for scientists. Make sure to mention the website for more info. Avoid technical jargon, keep it straightforward. Check for clarity and flow. Ensure all key points from both results are covered without redundancy.\\n</think>\\n\\nopenBIS is a **FAIR data management platform** that combines an **Electronic Laboratory Notebook (ELN)**, **Inventory Management System**, and **Laboratory Information Management System (LIMS)**. Developed by ETH Zurich since 2007, it initially focused on managing large-scale \"omics\" data from systems biology projects but has since evolved to emphasize ELN and inventory modules. \\n\\nKey features include:\\n- Enabling scientists to document experiments, store raw/derived data, and link it to materials, samples, and protocols.\\n- Supporting the **full data life cycle** (from project inception to data sharing).\\n- Ensuring compliance with **FAIR data principles** (Findable, Accessible, Interoperable, Reusable).\\n- Being **open-source** and used in both large-scale research and smaller laboratories.\\n\\nFor more details, visit the [openBIS website](https://sis.id.ethz.ch/services/rdm/openbis.html).', images=None, tool_calls=None)}, id='run-c670fc30-e662-4832-832f-85d3a1dada23-0', usage_metadata={'input_tokens': 541, 'output_tokens': 432, 'total_tokens': 973})]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = agent_executor.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"whats openBIS?\")]}\n",
    ")\n",
    "response[\"messages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d28e971",
   "metadata": {},
   "source": [
    "## Streaming Messages\n",
    "We've seen how the agent can be called with .invoke to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "61445caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "whats the weather in sf?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search_results_json (e07ad3db-0745-4560-b2a4-49b4d025f852)\n",
      " Call ID: e07ad3db-0745-4560-b2a4-49b4d025f852\n",
      "  Args:\n",
      "    query: current weather in San Francisco\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search_results_json\n",
      "\n",
      "[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1746023146, 'localtime': '2025-04-30 07:25'}, 'current': {'last_updated_epoch': 1746022500, 'last_updated': '2025-04-30 07:15', 'temp_c': 11.7, 'temp_f': 53.1, 'is_day': 1, 'condition': {'text': 'Partly cloudy', 'icon': '//cdn.weatherapi.com/weather/64x64/day/116.png', 'code': 1003}, 'wind_mph': 5.6, 'wind_kph': 9.0, 'wind_degree': 228, 'wind_dir': 'SW', 'pressure_mb': 1016.0, 'pressure_in': 29.99, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 86, 'cloud': 75, 'feelslike_c': 10.8, 'feelslike_f': 51.5, 'windchill_c': 9.6, 'windchill_f': 49.3, 'heatindex_c': 10.3, 'heatindex_f': 50.5, 'dewpoint_c': 9.5, 'dewpoint_f': 49.2, 'vis_km': 13.0, 'vis_miles': 8.0, 'uv': 0.1, 'gust_mph': 7.7, 'gust_kph': 12.5}}\"}, {\"url\": \"https://weathershogun.com/weather/usa/ca/san-francisco/480/april/2025-04-30\", \"content\": \"San Francisco, California Weather: Wednesday, April 30, 2025. Sunny weather, clear skies and sunny weather. Day 66°. Night 54°.\"}]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, let's see. The user asked for the weather in San Francisco. I used the Tavily search tool to get the latest info. The first result from WeatherAPI shows the current temperature is 11.7°C (53.1°F) with partly cloudy conditions. The second source mentions sunny weather with day temperatures at 66° and night at 54°. Wait, there's a discrepancy here. The WeatherAPI data is for April 30, 2025, at 7:25 AM, showing cooler temps, while the second link says day is 66°. Maybe the second source is a forecast or a different time. I should check the dates and times. The WeatherAPI entry has localtime as 2025-04-30 07:25, so that's the current reading. The other URL's title mentions April 30, 2025, but the content says day 66°, which might be a typo or different measurement. Since the user is asking for the current weather, I should rely on the WeatherAPI data. Also, the first source has more detailed metrics like wind, humidity, etc., which adds credibility. The second source seems less detailed and might be a static page. So, the accurate answer is the 11.7°C with partly cloudy from WeatherAPI. I'll present that and mention the slight difference in the second source, but clarify that the first is the current data.\n",
      "</think>\n",
      "\n",
      "The current weather in San Francisco is **partly cloudy** with a temperature of **11.7°C (53.1°F)**. Here's the detailed breakdown:\n",
      "\n",
      "- **Last Updated**: 7:15 AM (April 30, 2025)  \n",
      "- **Wind**: 5.6 mph (9.0 kph) from the southwest  \n",
      "- **Humidity**: 86%  \n",
      "- **Pressure**: 1016.0 mb (29.99 inches)  \n",
      "- **Visibility**: 13 km (8 miles)  \n",
      "\n",
      "A second source mentions **sunny weather** with daytime highs around **66°F** and nighttime lows at **54°F**. This might reflect a forecast or a different measurement time. For real-time accuracy, the WeatherAPI data above is more reliable. 🌤️\n"
     ]
    }
   ],
   "source": [
    "for step in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "99f39389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user asked for the weather in San Francisco. I called the Tavily search function with \"current weather in San Francisco\" and got two results. Let me check these.\n",
      "\n",
      "The first result is from weathershogun.com, mentioning April 30, 2025. It says sunny with day 66° and night 54°. The second link is from world-weather.info, also talking about April 2025, with similar temperatures: +52° and +63°, clear sky. \n",
      "\n",
      "Wait, the user didn't specify a date. The current date is October 2023, but the search results are for April 2025. That's a future date. Maybe the search tool defaulted to next year's forecast? The user might be asking for the current weather, but the results are for next year. That's a problem. \n",
      "\n",
      "I need to clarify. Since the results are for 2025, which is not the present, perhaps there was a misunderstanding. The user might have meant the current weather, but the search returned a forecast. Maybe the search query needs adjustment. Alternatively, the tool might not have real-time data. \n",
      "\n",
      "But the user's question is \"whats the weather in sf?\" without a date. The answer should be current, but the provided results are for next year. This might be an error in the search results. However, since I have to work with the given data, I should present the information from the search, noting the date discrepancy. \n",
      "\n",
      "So, the response should mention the dates from the search results and the weather details, but also note that the information is for 2025. The user might need to check the current date or maybe there was a mistake in the search. Alternatively, the search might have returned a forecast by default. \n",
      "\n",
      "In any case, the answer should be based on the provided tool response. So, I'll state the weather for San Francisco as per the search results, including the dates and temperatures, and mention that the information is for April 2025.\n",
      "</think>\n",
      "\n",
      "The weather in San Francisco for **April 30, 2025** is expected to be **sunny** with daytime highs around **66°F** and nighttime lows around **54°F**. Clear skies are predicted for the day. \n",
      "\n",
      "Note: These results appear to be a forecast for April 2025 rather than current conditions. For real-time weather, ensure the date in your query matches the current date. 🌞|"
     ]
    }
   ],
   "source": [
    "for step, metadata in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"whats the weather in sf?\")]},\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if metadata[\"langgraph_node\"] == \"agent\" and (text := step.text()):\n",
    "        print(text, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2780911c",
   "metadata": {},
   "source": [
    "## Adding in memory\n",
    "As mentioned earlier, this agent is stateless. This means it does not remember previous interactions. To give it memory we need to pass in a checkpointer. When passing in a checkpointer, we also have to pass in a thread_id when invoking the agent (so it knows which thread/conversation to resume from)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "02a20b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ad9aedd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = create_react_agent(model, tools, checkpointer=memory)\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "438c40b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='<think>\\nOkay, the user said, \"hi im bob!\" I need to respond appropriately. Let me check the tools provided. The only tool available is tavily_search_results_json, which is for search queries. Since the user is just greeting me and introducing themselves as Bob, there\\'s no need to use the search function here. I should respond with a friendly greeting and ask how they\\'re doing. No tool call is necessary in this case.\\n</think>\\n\\nHello, Bob! How are you today? 😊', additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-04-30T14:26:23.146792573Z', 'done': True, 'done_reason': 'stop', 'total_duration': 619481370, 'load_duration': 15752858, 'prompt_eval_count': 175, 'prompt_eval_duration': 10590130, 'eval_count': 104, 'eval_duration': 591701636, 'message': Message(role='assistant', content='<think>\\nOkay, the user said, \"hi im bob!\" I need to respond appropriately. Let me check the tools provided. The only tool available is tavily_search_results_json, which is for search queries. Since the user is just greeting me and introducing themselves as Bob, there\\'s no need to use the search function here. I should respond with a friendly greeting and ask how they\\'re doing. No tool call is necessary in this case.\\n</think>\\n\\nHello, Bob! How are you today? 😊', images=None, tool_calls=None)}, id='run-62c2140d-2b68-421c-b662-1ad245b9fce2-0', usage_metadata={'input_tokens': 175, 'output_tokens': 104, 'total_tokens': 279})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"hi im bob!\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c91c057a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='<think>\\nOkay, the user asked, \"whats my name?\" Let me check the conversation history. Earlier, the user introduced themselves as Bob. So the answer should be Bob. No need to use any tools here since the information is already available in the history. I\\'ll just respond with their name.\\n</think>\\n\\nYour name is Bob! 😊', additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-04-30T14:26:23.630608634Z', 'done': True, 'done_reason': 'stop', 'total_duration': 460960761, 'load_duration': 14792800, 'prompt_eval_count': 293, 'prompt_eval_duration': 13170195, 'eval_count': 72, 'eval_duration': 417777023, 'message': Message(role='assistant', content='<think>\\nOkay, the user asked, \"whats my name?\" Let me check the conversation history. Earlier, the user introduced themselves as Bob. So the answer should be Bob. No need to use any tools here since the information is already available in the history. I\\'ll just respond with their name.\\n</think>\\n\\nYour name is Bob! 😊', images=None, tool_calls=None)}, id='run-438b0ee8-a853-42af-9fcd-928d62835cb2-0', usage_metadata={'input_tokens': 293, 'output_tokens': 72, 'total_tokens': 365})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"whats my name?\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8099ee2",
   "metadata": {},
   "source": [
    "If you want to start a new conversation, all you have to do is change the thread_id used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3d0f32e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='<think>\\nOkay, the user is asking, \"whats my name?\" I need to figure out how to respond. Let me check the tools provided. The only tool available is tavily_search_results_json, which is for searching the web. But the user is asking for their own name, which isn\\'t something that can be found through a search. The assistant doesn\\'t have access to the user\\'s personal information unless the user provides it. Since there\\'s no function to retrieve the user\\'s name, I should inform them that I don\\'t have that information and ask them to provide it. I can\\'t use the search tool here because it\\'s not relevant. So the correct response is to prompt the user for their name.\\n</think>\\n\\nI don\\'t have access to your personal information. Could you please tell me your name?', additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-04-30T14:26:24.680476378Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1029115092, 'load_duration': 18629602, 'prompt_eval_count': 176, 'prompt_eval_duration': 10273674, 'eval_count': 167, 'eval_duration': 998842066, 'message': Message(role='assistant', content='<think>\\nOkay, the user is asking, \"whats my name?\" I need to figure out how to respond. Let me check the tools provided. The only tool available is tavily_search_results_json, which is for searching the web. But the user is asking for their own name, which isn\\'t something that can be found through a search. The assistant doesn\\'t have access to the user\\'s personal information unless the user provides it. Since there\\'s no function to retrieve the user\\'s name, I should inform them that I don\\'t have that information and ask them to provide it. I can\\'t use the search tool here because it\\'s not relevant. So the correct response is to prompt the user for their name.\\n</think>\\n\\nI don\\'t have access to your personal information. Could you please tell me your name?', images=None, tool_calls=None)}, id='run-572528b5-461e-4e9b-9335-da9aa2abf96d-0', usage_metadata={'input_tokens': 176, 'output_tokens': 167, 'total_tokens': 343})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"xyz123\"}}\n",
    "for chunk in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"whats my name?\")]}, config\n",
    "):\n",
    "    print(chunk)\n",
    "    print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
