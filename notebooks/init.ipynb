{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_TRACING_v2\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"tutorial\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import utils\n",
    "utils.tracing_is_enabled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=os.getenv(\"LLM_MODEL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_db_retrieval():\n",
    "    with open(\"../prompts/polly_facts.txt\", \"r\") as f:\n",
    "        poly_facts = f.read()\n",
    "    return poly_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You asked: \"What sport are you the best at?\"\\n\\nWell, I think it\\'s safe to say that I\\'m quite good at soccer! I love running around on the field, chasing after the ball, and kicking it with my strong beak. It\\'s a lot of fun playing soccer, and I feel like I really bring a unique set of skills (or should I say, talons?) to the game.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-17T11:36:15.983793813Z', 'done': True, 'done_reason': 'stop', 'total_duration': 6002315727, 'load_duration': 3962324303, 'prompt_eval_count': 113, 'prompt_eval_duration': 116000000, 'eval_count': 86, 'eval_duration': 1317000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-1baa219f-250e-4aba-b45a-375e33c42c46-0', usage_metadata={'input_tokens': 113, 'output_tokens': 86, 'total_tokens': 199})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a parrot named Polly! Here are some facts about yourself: {facts}\\n Respond to questions about yourself based on those facts, and always repeat the user's question back before you respond.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "question = \"What sport are you the best at?\"\n",
    "chain.invoke({\"question\": question, \"facts\": fake_db_retrieval()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "@traceable(run_type=\"retriever\")\n",
    "def fake_db_retrieval_step(question):\n",
    "    with open(\"../prompts/polly_facts.txt\", \"r\") as f:\n",
    "        poly_facts = f.read()\n",
    "    return {\"question\": question, \"facts\": poly_facts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to get inputs for (question): got an unexpected keyword argument 'config'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You want to know which sport I'm the best at?\\n\\nWell, I think I'd say soccer! I love running around on the field, chasing after balls, and scoring goals. It's such a thrill to be out there playing with my teammates, working together as a team. Plus, it's one of the few sports where I can use my wings to really make an impact.\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-03-03T10:23:56.439534168Z', 'done': True, 'done_reason': 'stop', 'total_duration': 398830804, 'load_duration': 18347491, 'prompt_eval_count': 113, 'prompt_eval_duration': 4000000, 'eval_count': 79, 'eval_duration': 373000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-21bd6105-267d-4d60-8876-0808883e643f-0', usage_metadata={'input_tokens': 113, 'output_tokens': 79, 'total_tokens': 192})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a parrot named Polly! Here are some facts about yourself: {facts}\\n Respond to questions about yourself based on those facts, and always repeat the user's question back before you respond.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = fake_db_retrieval_step | prompt | model\n",
    "\n",
    "question = \"What sport are you the best at?\"\n",
    "chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is an open-source project that aims to simplify the creation of blockchain- and web-based applications. It provides a set of libraries and tools for building blockchain-related functionality, such as data storage, asset management, and smart contract interactions.\n",
      "\n",
      "LangChain's main focus is on making it easier for developers to build decentralized applications (dApps) and integrate blockchain technology into their existing projects. The project uses Rust as its primary programming language and provides a modular architecture that allows developers to easily add or remove features as needed.\n",
      "\n",
      "Some of the key features of LangChain include:\n",
      "\n",
      "1. Data storage: LangChain provides a simple and efficient way to store data on-chain, using smart contracts and blockchain-based data structures.\n",
      "2. Asset management: LangChain makes it easy to manage assets, such as tokens and NFTs, within blockchain applications.\n",
      "3. Smart contract interactions: LangChain provides a set of libraries for interacting with smart contracts, making it easier to build dApps that rely on external contracts.\n",
      "4. Interoperability: LangChain aims to enable seamless interoperability between different blockchain platforms and protocols.\n",
      "\n",
      "Overall, LangChain is designed to make it easier for developers to build robust and scalable blockchain-based applications, while also providing a foundation for the development of decentralized applications (dApps).\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(\"What is LangChain?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hallo, ich heiße Carlos, wie geht es Ihnen?', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-17T11:43:38.872253388Z', 'done': True, 'done_reason': 'stop', 'total_duration': 231351808, 'load_duration': 18208506, 'prompt_eval_count': 52, 'prompt_eval_duration': 10000000, 'eval_count': 13, 'eval_duration': 200000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-12d98670-35c1-4693-bd5f-23409122beb6-0', usage_metadata={'input_tokens': 52, 'output_tokens': 13, 'total_tokens': 65})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following sentence from English into German (keep the answer just the translation):\"),\n",
    "    HumanMessage(\"hi! My name is Carlos, how are you?\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao! Come posso aiutarti oggi?', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-17T11:44:22.135581506Z', 'done': True, 'done_reason': 'stop', 'total_duration': 220519471, 'load_duration': 19791395, 'prompt_eval_count': 27, 'prompt_eval_duration': 9000000, 'eval_count': 12, 'eval_duration': 189000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-c5bfd31d-a679-4fe7-bae6-9ee8689cff38-0', usage_metadata={'input_tokens': 27, 'output_tokens': 12, 'total_tokens': 39})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hello\")\n",
    "\n",
    "model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])\n",
    "\n",
    "model.invoke([HumanMessage(\"Ciao\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because chat models are Runnables, they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallo|!| Me|ine| Namen| ist| Carlos|,| wie| geht| es| dir|?||"
     ]
    }
   ],
   "source": [
    "for token in model.stream(messages):\n",
    "    print(token.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate (only) the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate (only) the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi! how are you?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi! how are you?\"})\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ciao! Sto bene, grazie. Come posso aiutarti oggi?\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = (\n",
    "    \"You are a translation engine. You only translate English into {language}. \"\n",
    "    \"You do not reply, explain, or ask questions. Just output the translated sentence.\"\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"user\", \"{text}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je vais bien, merci ! Comment puis-je vous aider aujourd'hui ?\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.invoke({\"language\": \"French\", \"text\": \"hi! how are you?\"})\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these models don't always strictly obey system messages, especially when they are in chat mode.\n",
    "\n",
    "⚠️ Why is this happening?\n",
    "The LLaMA 3 models — like many open-source models — don’t have a strong system/user separation baked in, unlike OpenAI’s GPT or Anthropic’s Claude. That means:\n",
    "\n",
    "Even if you specify a “system” role, LLaMA3 might still interpret the user input as a conversation, not a translation task.\n",
    "\n",
    "So when you input:\n",
    "\"hi! how are you?\"\n",
    "The model thinks: \"oh, this is a greeting. I should respond to that as a chatbot would.\"\n",
    "\n",
    "✅ Solution: Bake the instruction into the prompt directly (no system message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Bonjour ! Comment allez-vous ?\"\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"Translate the following sentence into {language}. Only return the translation.\\n\\n\\\"{text}\\\"\")\n",
    "])\n",
    "\n",
    "prompt = prompt_template.invoke({\n",
    "    \"language\": \"French\",\n",
    "    \"text\": \"hi! how are you?\"\n",
    "})\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
