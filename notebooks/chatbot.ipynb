{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5d0724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_TRACING_v2\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"tutorial_chatbot\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bb5b13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ae553a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import utils\n",
    "utils.tracing_is_enabled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ab8fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=os.getenv(\"LLM_MODEL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b3a2914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hi Bob! It's nice to meet you. Is there something I can help you with, or would you like to chat for a bit?\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-22T14:42:28.082193655Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3056365505, 'load_duration': 2835813179, 'prompt_eval_count': 30, 'prompt_eval_duration': 65000000, 'eval_count': 30, 'eval_duration': 152000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-3b92133f-0b38-422a-92ed-8367c1df8ce2-0', usage_metadata={'input_tokens': 30, 'output_tokens': 30, 'total_tokens': 60})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54ecdb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have any information about you, so I don't know your name. We just started our conversation, and I'm here to help with any questions or topics you'd like to discuss. What would you like to talk about?\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-22T14:42:54.800872843Z', 'done': True, 'done_reason': 'stop', 'total_duration': 279870139, 'load_duration': 19847665, 'prompt_eval_count': 30, 'prompt_eval_duration': 8000000, 'eval_count': 49, 'eval_duration': 250000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-a307848f-24f4-4e63-8c19-2cc47770d507-0', usage_metadata={'input_tokens': 30, 'output_tokens': 49, 'total_tokens': 79})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae40fc9a",
   "metadata": {},
   "source": [
    "We can see that it doesn't take the previous conversation turn into context, and cannot answer the question. This makes for a terrible chatbot experience!\n",
    "To get around this, we need to pass the entire conversation history into the model. Let's see what happens when we do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb058725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I remember! Your name is Bob. You told me that earlier! Is there something on your mind that you'd like to chat about, or would you like some fun conversation starters instead?\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-22T14:44:19.513032688Z', 'done': True, 'done_reason': 'stop', 'total_duration': 225511627, 'load_duration': 22240100, 'prompt_eval_count': 55, 'prompt_eval_duration': 5000000, 'eval_count': 39, 'eval_duration': 195000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-b60e0b8d-2b61-4b57-be58-a37b9f498dc2-0', usage_metadata={'input_tokens': 55, 'output_tokens': 39, 'total_tokens': 94})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
    "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac823c",
   "metadata": {},
   "source": [
    "LangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\n",
    "\n",
    "Wrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\n",
    "\n",
    "LangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail, including how to use different persistence backends (e.g., SQLite or Postgres)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54c29f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb3a159",
   "metadata": {},
   "source": [
    "We now need to create a config that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a thread_id. This should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0693c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
